{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6czvz5VKO5M"
      },
      "source": [
        "# Notebook for Programming in Problem 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o8HI5JqTvU5"
      },
      "source": [
        "## Learning Objectives\n",
        "In this problem, we will use [PyTorch](https://pytorch.org/) to implement long short-term memory (LSTM) for named entity recognition (NER). We will use the same dataset and boilerplate code as in Programming Problem 1 of Assignment #3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObrHyvWvTyGZ"
      },
      "source": [
        "## Writing Code\n",
        "Look for the keyword \"TODO\" and fill in your code in the empty space.\n",
        "Feel free to change function signatures, but be careful that you might need to also change how they are called in other parts of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r6YTnpgbFdMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60dc5ed7-32e9-4a6d-8c7e-9ee4ab6d9b5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May  5 21:12:45 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi # you may need to try reconnecting to get a T4 gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnYMKJlKNXYe"
      },
      "source": [
        "## Installing PyTorch and Other Packages\n",
        "\n",
        "Install PyTorch using pip. See [https://pytorch.org/](https://pytorch.org/) if you want to install it on your computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-dRVuiP_JVdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94e470cb-6085-46b1-dac1-9d1ed3cef34f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchtext -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPsFH637OpLy"
      },
      "source": [
        "Test if our installation works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "c62StNb2NvKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d491c68-54ab-41f5-a373-0a24da53acba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch successfully installed!\n",
            "Version: 2.2.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Multiply two matrices on GPU\n",
        "a = torch.rand(100, 200).cuda()\n",
        "b = torch.rand(200, 100).cuda()\n",
        "c = torch.matmul(a, b)\n",
        "\n",
        "print(\"PyTorch successfully installed!\")\n",
        "print(\"Version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qaC8sxcqkGX"
      },
      "source": [
        "Also install [scikit-learn](https://scikit-learn.org/stable/). We will use it for calculating evaluation metrics such as accuracy and F1 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i5Y2xB_uqqM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "190eb31d-2060-4022-d02e-0b115766a967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhV4CYivRbt4"
      },
      "source": [
        "Let's import all the packages at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EjRM4cCFRh-d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import Vocab, vocab\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Dict, Optional, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn1bIPjAN-9V"
      },
      "source": [
        "## Long Short Term Memory (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJOKIneRTrTH"
      },
      "source": [
        "### Data Loading\n",
        "\n",
        "We will use the same dataset for named entity recognition in Assignment #2. First download the data and take a look at the first 50 lines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lWqz7kDxSqeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac7822a8-a1bd-45d6-ea63-abb994bf502a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EU NNP I-NP ORG\n",
            "rejects VBZ I-VP O\n",
            "German JJ I-NP MISC\n",
            "call NN I-NP O\n",
            "to TO I-VP O\n",
            "boycott VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Peter NNP I-NP PER\n",
            "Blackburn NNP I-NP PER\n",
            "\n",
            "BRUSSELS NNP I-NP LOC\n",
            "1996-08-22 CD I-NP O\n",
            "\n",
            "The DT I-NP O\n",
            "European NNP I-NP ORG\n",
            "Commission NNP I-NP ORG\n",
            "said VBD I-VP O\n",
            "on IN I-PP O\n",
            "Thursday NNP I-NP O\n",
            "it PRP B-NP O\n",
            "disagreed VBD I-VP O\n",
            "with IN I-PP O\n",
            "German JJ I-NP MISC\n",
            "advice NN I-NP O\n",
            "to TO I-PP O\n",
            "consumers NNS I-NP O\n",
            "to TO I-VP O\n",
            "shun VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            "until IN I-SBAR O\n",
            "scientists NNS I-NP O\n",
            "determine VBP I-VP O\n",
            "whether IN I-SBAR O\n",
            "mad JJ I-NP O\n",
            "cow NN I-NP O\n",
            "disease NN I-NP O\n",
            "can MD I-VP O\n",
            "be VB I-VP O\n",
            "transmitted VBN I-VP O\n",
            "to TO I-PP O\n",
            "sheep NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Germany NNP I-NP LOC\n",
            "'s POS B-NP O\n",
            "representative NN I-NP O\n"
          ]
        }
      ],
      "source": [
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\n",
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\n",
        "!cat eng.train | head -n 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVt1a6nzWsiF"
      },
      "source": [
        "Each line corresponds to a word. Different sentences are separated by an additional line break. Take \"EU NNP I-NP ORG\" as an example. \"EU\" is a word. \"NNP\" and \"I-NP\" are tags for POS tagging and chunking, which we will ignore. \"ORG\" is the tag for NER, which is our prediction target. There are 5 possible values for the NER tag: ORG, PER, LOC, MISC, and O.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WnNfOBUYJvVW"
      },
      "outputs": [],
      "source": [
        "# A sentence is a list of (word, tag) tuples.\n",
        "# For example, [(\"hello\", \"O\"), (\"world\", \"O\"), (\"!\", \"O\")]\n",
        "Sentence = List[Tuple[str, str]]\n",
        "\n",
        "\n",
        "def read_data_file(\n",
        "    datapath: str,\n",
        ") -> Tuple[List[Sentence], Dict[str, int], Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Read and preprocess input data from the file `datapath`.\n",
        "    Example:\n",
        "    ```\n",
        "        sentences, word_cnt, tag_cnt = read_data_file(\"eng.train\")\n",
        "    ```\n",
        "    Return values:\n",
        "        `sentences`: a list of sentences, including words and NER tags\n",
        "        `word_cnt`: a Counter object, the number of occurrences of each word\n",
        "        `tag_cnt`: a Counter object, the number of occurences of each NER tag\n",
        "    \"\"\"\n",
        "    sentences: List[Sentence] = []\n",
        "    word_cnt: Dict[str, int] = Counter()\n",
        "    tag_cnt: Dict[str, int] = Counter()\n",
        "\n",
        "    for sentence_txt in open(datapath).read().split(\"\\n\\n\"):\n",
        "        if \"DOCSTART\" in sentence_txt:\n",
        "            # Ignore dummy sentences at the begining of each document.\n",
        "            continue\n",
        "        # Read a new sentence\n",
        "        sentences.append([])\n",
        "        for token in sentence_txt.split(\"\\n\"):\n",
        "            w, _, _, t = token.split()\n",
        "            # Replace all digits with \"0\" to reduce out-of-vocabulary words\n",
        "            w = re.sub(\"\\d\", \"0\", w)\n",
        "            word_cnt[w] += 1\n",
        "            tag_cnt[t] += 1\n",
        "            sentences[-1].append((w, t))\n",
        "\n",
        "    return sentences, word_cnt, tag_cnt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WLMGYSZ7KxzP"
      },
      "outputs": [],
      "source": [
        "# Some helper code\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"\n",
        "    Use GPU when it is available; use CPU otherwise.\n",
        "    See https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code\n",
        "    \"\"\"\n",
        "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wVHAOb7iMPwC"
      },
      "outputs": [],
      "source": [
        "def eval_metrics(ground_truth: List[int], predictions: List[int]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate various evaluation metrics such as accuracy and F1 score\n",
        "    Parameters:\n",
        "        `ground_truth`: the list of ground truth NER tags\n",
        "        `predictions`: the list of predicted NER tags\n",
        "    \"\"\"\n",
        "    f1_scores = f1_score(ground_truth, predictions, average=None)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(ground_truth, predictions),\n",
        "        \"f1\": f1_scores,\n",
        "        \"average f1\": np.mean(f1_scores),\n",
        "        \"confusion matrix\": confusion_matrix(ground_truth, predictions),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s830dhbnj1L"
      },
      "source": [
        "## Long Short-term Memory (LSTM)\n",
        "\n",
        "Now we implement an one-layer LSTM for the same task and compare it to FFNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to7DnWNiY5ZS"
      },
      "source": [
        "### Data Loading **(4 points)**\n",
        "\n",
        "Like before, we first implement the data loader. But unlike before, each data example is now a variable-length sentence. How can we pack multiple sentences with different lengths into the same batch? One possible solution is to pad them to the same length using a special token. The code below illustrates the idea:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "J5oVgqE7JaJp"
      },
      "outputs": [],
      "source": [
        "# 3 sentences with different lengths\n",
        "sentence_1 = torch.tensor([6, 1, 2])\n",
        "sentence_2 = torch.tensor([4, 2, 7, 7, 9])\n",
        "sentence_3 = torch.tensor([3, 4])\n",
        "# Form a batch by padding 0\n",
        "sentence_batch = torch.tensor([\n",
        "    [6, 1, 2, 0, 0],\n",
        "    [4, 2, 7, 7, 9],\n",
        "    [3, 4, 0, 0, 0],\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udC0SMjkKaCN"
      },
      "source": [
        "We implement the above idea in a customized batching function `form_batch`. Optionally, see [here](https://pytorch.org/docs/stable/data.html#loading-batched-and-non-batched-data) for how batching works in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sACcGN4XYMgj"
      },
      "outputs": [],
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each data example is a sentence, including its words and NER tags.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, datapath: str, words_vocab: Optional[Vocab] = None, tags_vocab: Optional[Vocab] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the dataset by reading from datapath.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.sentences: List[Sentence] = []\n",
        "        UNKNOWN = \"<UNKNOWN>\"\n",
        "        PAD = \"<PAD>\"  # Special token used for padding\n",
        "\n",
        "        print(\"Loading data from %s\" % datapath)\n",
        "        self.sentences, word_cnt, tag_cnt = read_data_file(datapath)\n",
        "        print(\"%d sentences loaded.\" % len(self.sentences))\n",
        "\n",
        "        if words_vocab is None:\n",
        "            words_vocab = vocab(word_cnt, specials=[PAD, UNKNOWN])\n",
        "            words_vocab.set_default_index(words_vocab[UNKNOWN])\n",
        "\n",
        "        self.words_vocab = words_vocab\n",
        "\n",
        "        self.unknown_idx = self.words_vocab[UNKNOWN]\n",
        "        self.pad_idx = self.words_vocab[PAD]\n",
        "\n",
        "        if tags_vocab is None:\n",
        "            tags_vocab = vocab(tag_cnt, specials=[])\n",
        "        self.tags_vocab = tags_vocab\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Sentence:\n",
        "        \"\"\"\n",
        "        Get the idx'th sentence in the dataset.\n",
        "        \"\"\"\n",
        "        return self.sentences[idx]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Return the number of sentences in the dataset.\n",
        "        \"\"\"\n",
        "        # TODO: Implement this method\n",
        "        # START HERE\n",
        "        #raise NotImplementedError\n",
        "        return len(self.sentences)\n",
        "        # END\n",
        "\n",
        "    def form_batch(self, sentences: List[Sentence]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        A customized function for batching a number of sentences together.\n",
        "        Different sentences have different lengths. Let max_len be the longest length.\n",
        "        When packing them into one tensor, we need to pad all sentences to max_len.\n",
        "        Return values:\n",
        "            `words`: a list in which each element itself is a list of words in a sentence\n",
        "            `word_idxs`: a batch_size x max_len tensor.\n",
        "                       word_idxs[i][j] is the index of the j'th word in the i'th sentence .\n",
        "            `tags`: a list in which each element itself is a list of tags in a sentence\n",
        "            `tag_idxs`: a batch_size x max_len tensor\n",
        "                      tag_idxs[i][j] is the index of the j'th tag in the i'th sentence.\n",
        "            `valid_mask`: a batch_size x max_len tensor\n",
        "                        valid_mask[i][j] is True if the i'th sentence has the j'th word.\n",
        "                        Otherwise, valid[i][j] is False.\n",
        "        \"\"\"\n",
        "        words: List[List[str]] = []\n",
        "        tags: List[List[str]] = []\n",
        "        max_len = -1  # length of the longest sentence\n",
        "        for sent in sentences:\n",
        "            words.append([])\n",
        "            tags.append([])\n",
        "            for w, t in sent:\n",
        "                words[-1].append(w)\n",
        "                tags[-1].append(t)\n",
        "            max_len = max(max_len, len(words[-1]))\n",
        "\n",
        "        batch_size = len(sentences)\n",
        "        word_idxs = torch.full(\n",
        "            (batch_size, max_len), fill_value=self.pad_idx, dtype=torch.int64\n",
        "        )\n",
        "        tag_idxs = torch.full_like(word_idxs, fill_value=self.tags_vocab[\"O\"])\n",
        "        valid_mask = torch.zeros_like(word_idxs, dtype=torch.bool)\n",
        "\n",
        "        ## TODO: Fill in the values in word_idxs, tag_idxs, and valid_mask\n",
        "        ## Caveat: There may be out-of-vocabulary words in validation data\n",
        "        ## See torchtext.vocab.Vocab: https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab\n",
        "        ## START HERE\n",
        "\n",
        "        #raise NotImplementedError\n",
        "        UNKNOWN = \"<UNKNOWN>\"\n",
        "        for i, (word_list, tag_list) in enumerate(zip(words, tags)):\n",
        "          for j, (word, tag) in enumerate(zip(word_list, tag_list)):\n",
        "            if word in self.words_vocab:\n",
        "                word_idxs[i][j] = self.words_vocab[word]\n",
        "                valid_mask[i][j] = True##\n",
        "            else:\n",
        "                word_idxs[i][j] = self.words_vocab[UNKNOWN]  # Assign index of OOV token\n",
        "            tag_idxs[i][j] = self.tags_vocab[tag]\n",
        "            #valid_mask[i][j] = True\n",
        "\n",
        "        # END\n",
        "\n",
        "        return {\n",
        "            \"words\": words,\n",
        "            \"word_idxs\": word_idxs,\n",
        "            \"tags\": tags,\n",
        "            \"tag_idxs\": tag_idxs,\n",
        "            \"valid_mask\": valid_mask,\n",
        "        }\n",
        "\n",
        "\n",
        "def create_sequence_dataloaders(\n",
        "    batch_size: int, shuffle: bool = True\n",
        ") -> Tuple[DataLoader, DataLoader, Vocab]:\n",
        "    \"\"\"\n",
        "    Create the dataloaders for training and validaiton.\n",
        "    \"\"\"\n",
        "    ds_train = SequenceDataset(\"eng.train\")\n",
        "    ds_val = SequenceDataset(\"eng.val\", words_vocab=ds_train.words_vocab, tags_vocab=ds_train.tags_vocab)\n",
        "    loader_train = DataLoader(\n",
        "        ds_train,\n",
        "        batch_size,\n",
        "        shuffle,\n",
        "        collate_fn=ds_train.form_batch,  # customized function for batching\n",
        "        drop_last=True,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    loader_val = DataLoader(\n",
        "        ds_val, batch_size, collate_fn=ds_val.form_batch, pin_memory=True\n",
        "    )\n",
        "    return loader_train, loader_val, ds_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2EcVxYuYvGv"
      },
      "source": [
        "Here is a simple sanity-check. Try to understand its output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TazmodGWYx2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26c59725-c323-48b5-f635-32afc1f776b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "Iterating on the training data..\n",
            "{'words': [['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '0000-00-00']], 'word_idxs': tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
            "        [11, 12,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [13, 14,  0,  0,  0,  0,  0,  0,  0]]), 'tags': [['ORG', 'O', 'MISC', 'O', 'O', 'O', 'MISC', 'O', 'O'], ['PER', 'PER'], ['LOC', 'O']], 'tag_idxs': tensor([[0, 1, 2, 1, 1, 1, 2, 1, 1],\n",
            "        [3, 3, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [4, 1, 1, 1, 1, 1, 1, 1, 1]]), 'valid_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
            "        [ True,  True, False, False, False, False, False, False, False],\n",
            "        [ True,  True, False, False, False, False, False, False, False]])}\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "def check_sequence_dataloader() -> None:\n",
        "    loader_train, _, _ = create_sequence_dataloaders(batch_size=3, shuffle=False)\n",
        "    print(\"Iterating on the training data..\")\n",
        "    for i, data_batch in enumerate(loader_train):\n",
        "        if i == 0:\n",
        "            print(data_batch)\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n",
        "check_sequence_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifk3i-obY8YB"
      },
      "source": [
        "### Implement the Model **(8 points)**\n",
        "\n",
        "Next, implement LSTM for predicting NER tags from input words. [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) is definitely useful. Further, it is tricky to handle sentences in the same batch with different lengths. Please read the PyTorch documentation in detail!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3V0NvQynZF8e"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Long short-term memory for NER\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, words_vocab: Vocab, tags_vocab:Vocab, d_emb: int, d_hidden: int, bidirectional: bool) -> None:\n",
        "        \"\"\"\n",
        "        Initialize an LSTM\n",
        "        Parameters:\n",
        "            `words_vocab`: vocabulary of words\n",
        "            `tags_vocab`: vocabulary of tags\n",
        "            `d_emb`: dimension of word embeddings (D)\n",
        "            `d_hidden`: dimension of the hidden layer (H)\n",
        "            `bidirectional`: true if LSTM should be bidirectional\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: Create the word embeddings (nn.Embedding),\n",
        "        #       the LSTM (nn.LSTM) and the output layer (nn.Linear).\n",
        "        #       Read the torch docs for additional guidance : https://pytorch.org/docs/stable\n",
        "        #       Note: Pay attention to the LSTM output shapes!\n",
        "        # START HERE\n",
        "        #raise NotImplementedError\n",
        "        self.embeddings = nn.Embedding(len(words_vocab), d_emb)\n",
        "        self.lstm = nn.LSTM(d_emb, d_hidden, bidirectional=bidirectional, batch_first=True)\n",
        "        self.fc = nn.Linear(d_hidden * 2 if bidirectional else d_hidden, len(tags_vocab))\n",
        "\n",
        "        # END\n",
        "\n",
        "    def forward(\n",
        "        self, word_idxs: torch.Tensor, valid_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Given words in sentences, predict the logits of the NER tag.\n",
        "        Parameters:\n",
        "            `word_idxs`: a batch_size x max_len tensor\n",
        "            `valid_mask`: a batch_size x max_len tensor\n",
        "        Return values:\n",
        "            `logits`: a batch_size x max_len x 5 tensor\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass\n",
        "        # START HERE\n",
        "        #raise NotImplementedError\n",
        "        word_embeddings = self.embeddings(word_idxs)\n",
        "        lstm_out, _ = self.lstm(word_embeddings)\n",
        "        logits = self.fc(lstm_out)\n",
        "        # END\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BFTKaB4Zydx"
      },
      "source": [
        "We do a sanity-check by loading a batch of data examples and pass it through the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PKg1ni4QZ6D1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37062d8c-dd6b-46e7-80ab-601127db1d77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (embeddings): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
            "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "Input word_idxs shape: torch.Size([4, 30])\n",
            "Input valid_mask shape: torch.Size([4, 30])\n",
            "Output logits shape: torch.Size([4, 30, 5])\n"
          ]
        }
      ],
      "source": [
        "def check_lstm() -> None:\n",
        "    # Hyperparameters\n",
        "    batch_size = 4\n",
        "    d_emb = 64\n",
        "    d_hidden = 128\n",
        "    bidirectional = True\n",
        "    # Create the dataloaders and the model\n",
        "    loader_train, _, ds_train = create_sequence_dataloaders(batch_size)\n",
        "    model = LSTM(ds_train.words_vocab, ds_train.tags_vocab, d_emb, d_hidden, bidirectional)\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # Get the first batch\n",
        "    data_batch = next(iter(loader_train))\n",
        "    # Move data to GPU\n",
        "    word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "    tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "    valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "    # Calculate the model\n",
        "    print(\"Input word_idxs shape:\", word_idxs.size())\n",
        "    print(\"Input valid_mask shape:\", valid_mask.size())\n",
        "    logits = model(word_idxs, valid_mask)\n",
        "    print(\"Output logits shape:\", logits.size())\n",
        "\n",
        "\n",
        "check_lstm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jddDYUiLY-hc"
      },
      "source": [
        "### Training and Validation **(6 points)**\n",
        "\n",
        "Complete the functions for training and validating the LSTM model. When calculating the loss function, you only want to include values from valid positions (where `valid_mask` is `True`). The `reduction` parameter in [F.cross_entropy](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy) may be useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hv_15mnXZ_dy"
      },
      "outputs": [],
      "source": [
        "def train_lstm(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    silent: bool = False,  # whether to print the training loss\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Train the LSTM model.\n",
        "    Return values:\n",
        "        1. the average training loss\n",
        "        2. training metrics such as accuracy and F1 score\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "    report_interval = 100\n",
        "\n",
        "    for i, data_batch in enumerate(loader):\n",
        "        word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "        tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "        valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "        # TODO: Do the same tasks as train_ffnn\n",
        "        # START HERE\n",
        "        # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n",
        "        #raise NotImplementedError\n",
        "        logits = model(word_idxs, valid_mask)\n",
        "        loss = F.cross_entropy(logits[valid_mask], tag_idxs[valid_mask])\n",
        "        # END\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # we get (unmasked) predictions by getting argmax of logits along last dimension (You will need to define logits!)\n",
        "        net_predictions = torch.argmax(logits, -1)\n",
        "\n",
        "        # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n",
        "        tag_idxs_flat = tag_idxs.flatten()\n",
        "        valid_mask_flat = valid_mask.flatten()\n",
        "        net_predictions_flat = net_predictions.flatten()\n",
        "\n",
        "        ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
        "        predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
        "\n",
        "        if not silent and i > 0 and i % report_interval == 0:\n",
        "            print(\n",
        "                \"\\t[%06d/%06d] Loss: %f\"\n",
        "                % (i, len(loader), np.mean(losses[-report_interval:]))\n",
        "            )\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def validate_lstm(\n",
        "    model: nn.Module, loader: DataLoader, device: torch.device\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Validate the model.\n",
        "    Return the validation loss and metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for data_batch in loader:\n",
        "            word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "            tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "            valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "            # TODO: Do the same tasks as validate_ffnn\n",
        "            # START HERE\n",
        "            # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n",
        "            #raise NotImplementedError\n",
        "            logits = model(word_idxs,valid_mask)\n",
        "            loss = F.cross_entropy(logits[valid_mask],tag_idxs[valid_mask])\n",
        "            # END\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # we get (unmasked) predictions by getting argmax of logits (You will need to define logits!)\n",
        "            net_predictions = torch.argmax(logits, -1)\n",
        "\n",
        "            # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n",
        "            tag_idxs_flat = tag_idxs.flatten()\n",
        "            valid_mask_flat = valid_mask.flatten()\n",
        "            net_predictions_flat = net_predictions.flatten()\n",
        "\n",
        "            ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
        "            predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def train_val_loop_lstm(hyperparams: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Train and validate the LSTM model for a number of epochs.\n",
        "    \"\"\"\n",
        "    print(\"Hyperparameters:\", hyperparams)\n",
        "    # Create the dataloaders\n",
        "    loader_train, loader_val, ds_train = create_sequence_dataloaders(\n",
        "        hyperparams[\"batch_size\"]\n",
        "    )\n",
        "    # Create the model\n",
        "    model = LSTM(\n",
        "        ds_train.words_vocab,\n",
        "        ds_train.tags_vocab,\n",
        "        hyperparams[\"d_emb\"],\n",
        "        hyperparams[\"d_hidden\"],\n",
        "        hyperparams[\"bidirectional\"],\n",
        "    )\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # Create the optimizer\n",
        "    optimizer = optim.RMSprop(\n",
        "        model.parameters(), hyperparams[\"learning_rate\"], weight_decay=hyperparams[\"l2\"]\n",
        "    )\n",
        "\n",
        "    # Train and validate\n",
        "    for i in range(hyperparams[\"num_epochs\"]):\n",
        "        print(\"Epoch #%d\" % i)\n",
        "\n",
        "        print(\"Training..\")\n",
        "        loss_train, metrics_train = train_lstm(model, loader_train, optimizer, device)\n",
        "        print(\"Training loss: \", loss_train)\n",
        "        print(\"Training metrics:\")\n",
        "        for k, v in metrics_train.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "        print(\"Validating..\")\n",
        "        loss_val, metrics_val = validate_lstm(model, loader_val, device)\n",
        "        print(\"Validation loss: \", loss_val)\n",
        "        print(\"Validation metrics:\")\n",
        "        for k, v in metrics_val.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU9Nef7yal_M"
      },
      "source": [
        "Run the experiment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pFxQxlokai6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb27096-56b6-4301-bd3b-08bbc5566b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'bidirectional': True, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (embeddings): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
            "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  0.7913342173452731\n",
            "Training metrics:\n",
            "\t accuracy :  0.7873443207926121\n",
            "\t f1 :  [0.23389619 0.88501083 0.05080133 0.20354312 0.25738091]\n",
            "\t average f1 :  0.3261264751296983\n",
            "\t confusion matrix :  [[  1683   7201    222    521    234]\n",
            " [  2145 152444   4469   7161    952]\n",
            " [   185   3707    252    249    136]\n",
            " [   305   8057    248   2183    135]\n",
            " [   212   5922    201    408   1421]]\n",
            "Validating..\n",
            "Validation loss:  0.24993539708001272\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9283904520602747\n",
            "\t f1 :  [0.55579554 0.9689248  0.35193945 0.62215591 0.66512244]\n",
            "\t average f1 :  0.6327876266183639\n",
            "\t confusion matrix :  [[  772   733    22   132   137]\n",
            " [   30 38975     8    89    38]\n",
            " [   70   457   186    41    71]\n",
            " [   40   586     2   834    25]\n",
            " [   70   559    14    98  1005]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.2277009332621539\n",
            "Training metrics:\n",
            "\t accuracy :  0.9309322795042213\n",
            "\t f1 :  [0.65345966 0.97281482 0.57419981 0.73159472 0.74995138]\n",
            "\t average f1 :  0.7364040750992349\n",
            "\t confusion matrix :  [[  5690   2549    274    721    633]\n",
            " [   367 165576    151    583    207]\n",
            " [   498   1346   2072    268    338]\n",
            " [   544   2613     71   7448    286]\n",
            " [   449   1438    127    379   5784]]\n",
            "Validating..\n",
            "Validation loss:  0.14199574823890412\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9604836200382273\n",
            "\t f1 :  [0.68839818 0.98674056 0.71801242 0.80577428 0.8333818 ]\n",
            "\t average f1 :  0.8064614460285304\n",
            "\t confusion matrix :  [[  982   393   103   159   159]\n",
            " [   16 38995    45    56    28]\n",
            " [    7   174   578    23    43]\n",
            " [    4   207    18  1228    30]\n",
            " [   48   129    41    95  1433]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.1169600036961061\n",
            "Training metrics:\n",
            "\t accuracy :  0.9657351081430978\n",
            "\t f1 :  [0.8096334  0.98889727 0.78651685 0.88458688 0.86721036]\n",
            "\t average f1 :  0.8673689530257336\n",
            "\t confusion matrix :  [[  7564   1032    308    456    458]\n",
            " [   298 165800    126    285    115]\n",
            " [   296    561   3325    133    199]\n",
            " [   352    820     55   9550    168]\n",
            " [   357    486    127    223   6965]]\n",
            "Validating..\n",
            "Validation loss:  0.10300704730408532\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9726407965506512\n",
            "\t f1 :  [0.79901659 0.99210231 0.79901051 0.8572333  0.88071801]\n",
            "\t average f1 :  0.8656161462596978\n",
            "\t confusion matrix :  [[ 1300   184    63   137   112]\n",
            " [   40 38942    44    92    22]\n",
            " [   26   104   646    17    32]\n",
            " [   15    81    16  1354    21]\n",
            " [   77    53    23    72  1521]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.06869360514813\n",
            "Training metrics:\n",
            "\t accuracy :  0.9806380572288407\n",
            "\t f1 :  [0.88588666 0.99450513 0.85648521 0.94548109 0.92121362]\n",
            "\t average f1 :  0.9207143427857467\n",
            "\t confusion matrix :  [[  8590    523    213    245    302]\n",
            " [   212 166690     89    127     47]\n",
            " [   253    348   3721     82    135]\n",
            " [   192    297     30  10362     89]\n",
            " [   273    199     97    133   7454]]\n",
            "Validating..\n",
            "Validation loss:  0.09033693532858576\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9769080321820687\n",
            "\t f1 :  [0.83323912 0.99335607 0.81545064 0.89109613 0.89568765]\n",
            "\t average f1 :  0.8857659229421053\n",
            "\t confusion matrix :  [[ 1474   155    53    31    83]\n",
            " [   45 39023    41    15    16]\n",
            " [   39    98   665     4    19]\n",
            " [   70   110    20  1256    31]\n",
            " [  114    42    27    26  1537]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.041059734092818365\n",
            "Training metrics:\n",
            "\t accuracy :  0.9889763700904157\n",
            "\t f1 :  [0.93459764 0.99690193 0.91139819 0.97503189 0.95205395]\n",
            "\t average f1 :  0.9539967228957125\n",
            "\t confusion matrix :  [[  9117    308    122    115    190]\n",
            " [   126 166522     66     52     29]\n",
            " [   146    227   3986     41     95]\n",
            " [    90    107     26  10700     49]\n",
            " [   179    120     52     68   7764]]\n",
            "Validating..\n",
            "Validation loss:  0.08509541302919388\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9795528292661244\n",
            "\t f1 :  [0.84387241 0.99448119 0.84863222 0.90449438 0.90525708]\n",
            "\t average f1 :  0.8993474567129057\n",
            "\t confusion matrix :  [[ 1508   128    46    29    85]\n",
            " [   59 39013    37    18    13]\n",
            " [   40    65   698     5    17]\n",
            " [   68    82    15  1288    34]\n",
            " [  103    31    24    21  1567]]\n",
            "Epoch #5\n",
            "Training..\n",
            "Training loss:  0.02634178419355993\n",
            "Training metrics:\n",
            "\t accuracy :  0.9934276239562108\n",
            "\t f1 :  [0.96002848 0.9981303  0.94692958 0.98793529 0.97012087]\n",
            "\t average f1 :  0.9726289029916698\n",
            "\t confusion matrix :  [[  9439    198     68     59    121]\n",
            " [    81 166560     38     19     19]\n",
            " [    97    144   4202     24     54]\n",
            " [    41     52      8  10809     24]\n",
            " [   121     73     38     37   7906]]\n",
            "Validating..\n",
            "Validation loss:  0.08572354433791977\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9807307641018802\n",
            "\t f1 :  [0.85222552 0.99433215 0.85851896 0.91366417 0.91376356]\n",
            "\t average f1 :  0.9065008740584706\n",
            "\t confusion matrix :  [[ 1436   158    52    53    97]\n",
            " [   29 39034    36    24    17]\n",
            " [   16    68   713     7    21]\n",
            " [   27    81    14  1344    21]\n",
            " [   66    32    21    27  1600]]\n",
            "Epoch #6\n",
            "Training..\n",
            "Training loss:  0.017609398846549017\n",
            "Training metrics:\n",
            "\t accuracy :  0.9959474162283022\n",
            "\t f1 :  [0.97351162 0.99880828 0.96647295 0.99477496 0.98217458]\n",
            "\t average f1 :  0.983148478153975\n",
            "\t confusion matrix :  [[  9574    137     57     24     84]\n",
            " [    50 166787     28     12     11]\n",
            " [    68     87   4324     10     27]\n",
            " [    25     21      2  10852     11]\n",
            " [    76     52     21      9   8017]]\n",
            "Validating..\n",
            "Validation loss:  0.08416334805744034\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9812197181846468\n",
            "\t f1 :  [0.85658043 0.99482103 0.86966967 0.91464261 0.91091378]\n",
            "\t average f1 :  0.9093255033848429\n",
            "\t confusion matrix :  [[ 1523   112    41    30    90]\n",
            " [   59 38994    41    27    19]\n",
            " [   30    51   724     5    15]\n",
            " [   55    72    11  1318    31]\n",
            " [   93    25    23    15  1590]]\n",
            "Epoch #7\n",
            "Training..\n",
            "Training loss:  0.011064017019062131\n",
            "Training metrics:\n",
            "\t accuracy :  0.9976278872748112\n",
            "\t f1 :  [0.98599838 0.9992046  0.9809228  0.99730014 0.98903522]\n",
            "\t average f1 :  0.990492229395411\n",
            "\t confusion matrix :  [[  9718     75     25     13     41]\n",
            " [    36 167079     14      4     14]\n",
            " [    32     66   4422      5     14]\n",
            " [    10     15      2  10897      7]\n",
            " [    44     42     14      3   8073]]\n",
            "Validating..\n",
            "Validation loss:  0.08737065217324666\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9819531493087967\n",
            "\t f1 :  [0.86325737 0.9949359  0.8767288  0.91397109 0.91827204]\n",
            "\t average f1 :  0.9134330403487331\n",
            "\t confusion matrix :  [[ 1553   113    39    32    59]\n",
            " [   59 38999    39    30    13]\n",
            " [   30    46   729     5    15]\n",
            " [   57    72    10  1328    20]\n",
            " [  103    25    21    24  1573]]\n",
            "Epoch #8\n",
            "Training..\n",
            "Training loss:  0.00736024934384558\n",
            "Training metrics:\n",
            "\t accuracy :  0.9985983359604143\n",
            "\t f1 :  [0.99184521 0.99953283 0.98782378 0.99853895 0.99369992]\n",
            "\t average f1 :  0.9942881364664723\n",
            "\t confusion matrix :  [[  9791     38     16      8     20]\n",
            " [    22 166884     13      3     10]\n",
            " [    22     42   4462      3      9]\n",
            " [     6      4      1  10935      5]\n",
            " [    29     24      4      2   8123]]\n",
            "Validating..\n",
            "Validation loss:  0.11208372030939374\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9779526159043428\n",
            "\t f1 :  [0.81579779 0.9937874  0.81267067 0.91030601 0.91247569]\n",
            "\t average f1 :  0.8890075118201102\n",
            "\t confusion matrix :  [[ 1291   197   139    33   136]\n",
            " [   22 39031    55    14    18]\n",
            " [    6    51   744     5    19]\n",
            " [   21   100    34  1294    38]\n",
            " [   29    31    34    10  1642]]\n",
            "Epoch #9\n",
            "Training..\n",
            "Training loss:  0.02511632263108536\n",
            "Training metrics:\n",
            "\t accuracy :  0.9932728605552259\n",
            "\t f1 :  [0.97440604 0.99689993 0.96566999 0.97398791 0.98280068]\n",
            "\t average f1 :  0.9787529096752989\n",
            "\t confusion matrix :  [[  9556    177     38     33     53]\n",
            " [   112 166575     84    195     73]\n",
            " [    38    102   4360     14     17]\n",
            " [    25    238     11  10634     25]\n",
            " [    26     55      6     27   8057]]\n",
            "Validating..\n",
            "Validation loss:  0.08235532311456543\n",
            "Validation metrics:\n",
            "\t accuracy :  0.983531137484998\n",
            "\t f1 :  [0.8693668  0.99553503 0.88151659 0.92806001 0.92427296]\n",
            "\t average f1 :  0.9197502775993789\n",
            "\t confusion matrix :  [[ 1524   108    51    32    81]\n",
            " [   48 39019    39    25     9]\n",
            " [   18    43   744     6    14]\n",
            " [   40    57    11  1361    18]\n",
            " [   80    21    18    22  1605]]\n",
            "Epoch #10\n",
            "Training..\n",
            "Training loss:  0.005532612894765205\n",
            "Training metrics:\n",
            "\t accuracy :  0.9989120236360369\n",
            "\t f1 :  [0.99529804 0.99952048 0.9914757  0.99826927 0.99583078]\n",
            "\t average f1 :  0.9960788533239618\n",
            "\t confusion matrix :  [[  9843     34      9      4      8]\n",
            " [    18 166753     19      7     12]\n",
            " [     8     31   4478      2      3]\n",
            " [     2     16      0  10959      4]\n",
            " [    10     23      5      3   8121]]\n",
            "Validating..\n",
            "Validation loss:  0.08585328502314431\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9834200115570965\n",
            "\t f1 :  [0.8668363  0.99541911 0.88983051 0.92898403 0.92228276]\n",
            "\t average f1 :  0.9206705423851556\n",
            "\t confusion matrix :  [[ 1533   108    40    30    85]\n",
            " [   62 39005    31    29    13]\n",
            " [   25    40   735     8    17]\n",
            " [   40    55     7  1367    18]\n",
            " [   81    21    14    22  1608]]\n",
            "Epoch #11\n",
            "Training..\n",
            "Training loss:  0.0032542284008943373\n",
            "Training metrics:\n",
            "\t accuracy :  0.9994710288488889\n",
            "\t f1 :  [0.99762374 0.99977831 0.99503585 0.99949645 0.9978521 ]\n",
            "\t average f1 :  0.9979572910122881\n",
            "\t confusion matrix :  [[  9866     14      8      3      4]\n",
            " [     8 166860      7      1      7]\n",
            " [     3     21   4510      0      3]\n",
            " [     3      3      0  10917      1]\n",
            " [     4     13      3      0   8130]]\n",
            "Validating..\n",
            "Validation loss:  0.09068093502095767\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9833311108147753\n",
            "\t f1 :  [0.86788155 0.99535631 0.88571429 0.92417545 0.92591525]\n",
            "\t average f1 :  0.9198085696390184\n",
            "\t confusion matrix :  [[ 1524   111    45    37    79]\n",
            " [   52 39011    40    27    10]\n",
            " [   22    41   744     7    11]\n",
            " [   40    61    10  1359    17]\n",
            " [   78    22    16    24  1606]]\n",
            "Epoch #12\n",
            "Training..\n",
            "Training loss:  0.002284791411107613\n",
            "Training metrics:\n",
            "\t accuracy :  0.9997157886863952\n",
            "\t f1 :  [0.99893385 0.99988325 0.99769509 0.99958891 0.99853229]\n",
            "\t average f1 :  0.9989266788837554\n",
            "\t confusion matrix :  [[  9838      7      4      3      0]\n",
            " [     3 167009      2      0      3]\n",
            " [     1      9   4545      1      2]\n",
            " [     1      2      0  10942      2]\n",
            " [     2     13      2      0   8164]]\n",
            "Validating..\n",
            "Validation loss:  0.09532302937337331\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9831755345157132\n",
            "\t f1 :  [0.8681635  0.99520518 0.88256228 0.92207792 0.92709529]\n",
            "\t average f1 :  0.9190208350392789\n",
            "\t confusion matrix :  [[ 1508   120    49    36    83]\n",
            " [   41 39021    41    26    11]\n",
            " [   18    43   744     7    13]\n",
            " [   40    72    10  1349    16]\n",
            " [   71    22    17    21  1615]]\n",
            "Epoch #13\n",
            "Training..\n",
            "Training loss:  0.0017400115676638153\n",
            "Training metrics:\n",
            "\t accuracy :  0.999800260659839\n",
            "\t f1 :  [0.99918946 0.99991007 0.99856083 0.99972578 0.99907891]\n",
            "\t average f1 :  0.9992930094172614\n",
            "\t confusion matrix :  [[  9862      6      1      2      1]\n",
            " [     3 166777      2      0      4]\n",
            " [     0      8   4510      0      1]\n",
            " [     1      1      1  10937      1]\n",
            " [     2      6      0      0   8135]]\n",
            "Validating..\n",
            "Validation loss:  0.09789674196924482\n",
            "Validation metrics:\n",
            "\t accuracy :  0.982864381917589\n",
            "\t f1 :  [0.86603827 0.99494962 0.88662268 0.92140184 0.92595794]\n",
            "\t average f1 :  0.9189940697139116\n",
            "\t confusion matrix :  [[ 1516   121    42    37    80]\n",
            " [   53 39007    38    31    11]\n",
            " [   20    45   739     7    14]\n",
            " [   41    71     8  1354    13]\n",
            " [   75    26    15    23  1607]]\n",
            "Epoch #14\n",
            "Training..\n",
            "Training loss:  0.0013806382603770882\n",
            "Training metrics:\n",
            "\t accuracy :  0.9998853519694538\n",
            "\t f1 :  [0.99964501 0.99994314 0.99911111 0.99981805 0.99951076]\n",
            "\t average f1 :  0.9996056158915664\n",
            "\t confusion matrix :  [[  9856      3      0      2      0]\n",
            " [     1 167077      4      0      2]\n",
            " [     0      3   4496      0      1]\n",
            " [     1      1      0  10990      0]\n",
            " [     0      5      0      0   8172]]\n",
            "Validating..\n",
            "Validation loss:  0.10183309337922505\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9828866071031693\n",
            "\t f1 :  [0.86526435 0.99495014 0.88915663 0.92213115 0.92551963]\n",
            "\t average f1 :  0.9194043796555421\n",
            "\t confusion matrix :  [[ 1522   122    41    34    77]\n",
            " [   56 39011    34    28    11]\n",
            " [   21    46   738     7    13]\n",
            " [   43    73     7  1350    14]\n",
            " [   80    26    15    22  1603]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "train_val_loop_lstm({\n",
        "    \"bidirectional\": True,\n",
        "    \"batch_size\": 512,\n",
        "    \"d_emb\": 64,\n",
        "    \"d_hidden\": 128,\n",
        "    \"num_epochs\": 15,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"l2\": 1e-6,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vA-Yjqg7n0V"
      },
      "source": [
        "We were using bidirectional LSTMs. Please re-run the experiment with a regular (unidirectional) LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7wNrdvJ98ARB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ccc20e2-866c-41cf-80b5-37d6bbb060a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'bidirectional': False, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (embeddings): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  0.5875688095887502\n",
            "Training metrics:\n",
            "\t accuracy :  0.8119453584604647\n",
            "\t f1 :  [0.11134106 0.90227312 0.10275229 0.27280387 0.37201148]\n",
            "\t average f1 :  0.35223636330816305\n",
            "\t confusion matrix :  [[   807   7297     40    923    806]\n",
            " [  3119 156391     84   6800    688]\n",
            " [   224   3385    252    295    341]\n",
            " [   208   7444     21   3076    203]\n",
            " [   265   5061     11    505   2334]]\n",
            "Validating..\n",
            "Validation loss:  0.23777392506599426\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9317909054540605\n",
            "\t f1 :  [0.41416474 0.97793357 0.46819338 0.65466748 0.69478908]\n",
            "\t average f1 :  0.6419496524284453\n",
            "\t confusion matrix :  [[  538   571    37   283   367]\n",
            " [   70 38778    19   196    77]\n",
            " [   54   274   276    94   127]\n",
            " [   35   324     5  1073    50]\n",
            " [  105   219    17   145  1260]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.21220509045653874\n",
            "Training metrics:\n",
            "\t accuracy :  0.9325469463078879\n",
            "\t f1 :  [0.60290179 0.97804337 0.5772112  0.76865882 0.72949756]\n",
            "\t average f1 :  0.731262546861436\n",
            "\t confusion matrix :  [[  5402   2016    425    890   1127]\n",
            " [   500 165304    157    706    294]\n",
            " [   694    938   2196    258    452]\n",
            " [   572   1880    111   8167    214]\n",
            " [   892    931    182    285   5902]]\n",
            "Validating..\n",
            "Validation loss:  0.13234451838902064\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9626616882250967\n",
            "\t f1 :  [0.69145427 0.99025004 0.71778351 0.83973693 0.81297602]\n",
            "\t average f1 :  0.8104401550186928\n",
            "\t confusion matrix :  [[ 1153   247    82    92   222]\n",
            " [   83 38950    32    37    38]\n",
            " [   70   117   557    21    60]\n",
            " [   77   144    15  1213    38]\n",
            " [  156    69    41    39  1441]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.1170469061643989\n",
            "Training metrics:\n",
            "\t accuracy :  0.9639606408326438\n",
            "\t f1 :  [0.77540791 0.99079861 0.7577155  0.89471273 0.8353529 ]\n",
            "\t average f1 :  0.8507975297637526\n",
            "\t confusion matrix :  [[  7580    814    356    446    702]\n",
            " [   377 166149    119    318    111]\n",
            " [   501    425   3204    140    251]\n",
            " [   417    592     89   9764    118]\n",
            " [   778    330    168    178   6687]]\n",
            "Validating..\n",
            "Validation loss:  0.10410867844309125\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9713739609725741\n",
            "\t f1 :  [0.75477239 0.99357405 0.79006211 0.88274484 0.84543656]\n",
            "\t average f1 :  0.8533179915048714\n",
            "\t confusion matrix :  [[ 1285   153    69    92   197]\n",
            " [   74 38964    32    38    32]\n",
            " [   49    74   636    24    42]\n",
            " [   57    67    12  1325    26]\n",
            " [  144    34    36    36  1496]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.07675720972043497\n",
            "Training metrics:\n",
            "\t accuracy :  0.9770443762210438\n",
            "\t f1 :  [0.84700484 0.99507616 0.82468056 0.94791619 0.88273817]\n",
            "\t average f1 :  0.899483183436122\n",
            "\t confusion matrix :  [[  8307    466    301    232    553]\n",
            " [   245 166626     73    138     72]\n",
            " [   384    256   3582    101    200]\n",
            " [   214    232     66  10383     56]\n",
            " [   606    167    142    102   7144]]\n",
            "Validating..\n",
            "Validation loss:  0.09800571203231812\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9738409565719873\n",
            "\t f1 :  [0.76463339 0.99450052 0.80830281 0.90433031 0.85082579]\n",
            "\t average f1 :  0.8645185636073938\n",
            "\t confusion matrix :  [[ 1241   132    78    69   276]\n",
            " [   64 38970    27    35    44]\n",
            " [   32    59   662    20    52]\n",
            " [   36    50    15  1347    39]\n",
            " [   77    20    31    21  1597]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.05326118016684497\n",
            "Training metrics:\n",
            "\t accuracy :  0.9841837676560543\n",
            "\t f1 :  [0.89027126 0.99709411 0.86785999 0.97071662 0.9134109 ]\n",
            "\t average f1 :  0.9278705781546839\n",
            "\t confusion matrix :  [[  8730    278    254    153    452]\n",
            " [   183 166589     58     51     38]\n",
            " [   298    159   3806     66    168]\n",
            " [   108    109     39  10674     38]\n",
            " [   426     95    117     80   7458]]\n",
            "Validating..\n",
            "Validation loss:  0.09656023819531713\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9753078188202872\n",
            "\t f1 :  [0.78297615 0.99455572 0.81945289 0.9110958  0.85907493]\n",
            "\t average f1 :  0.873431097567703\n",
            "\t confusion matrix :  [[ 1297   139    67    54   239]\n",
            " [   53 39002    32    18    35]\n",
            " [   27    65   674     8    51]\n",
            " [   46    64    17  1322    38]\n",
            " [   94    21    30    13  1588]]\n",
            "Epoch #5\n",
            "Training..\n",
            "Training loss:  0.03978500377248834\n",
            "Training metrics:\n",
            "\t accuracy :  0.9880062359603727\n",
            "\t f1 :  [0.9139889  0.99778612 0.90069647 0.98272955 0.93131919]\n",
            "\t average f1 :  0.945304046223548\n",
            "\t confusion matrix :  [[  8974    243    211     84    372]\n",
            " [   151 166983     40     28     28]\n",
            " [   204    131   4009     36    144]\n",
            " [    72     61     24  10783     25]\n",
            " [   352     59     94     49   7614]]\n",
            "Validating..\n",
            "Validation loss:  0.09232775494456291\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9768413566253278\n",
            "\t f1 :  [0.7983801  0.99465282 0.82466747 0.91759919 0.87811712]\n",
            "\t average f1 :  0.8826833385672833\n",
            "\t confusion matrix :  [[ 1380   128    60    56   172]\n",
            " [   83 38970    36    26    25]\n",
            " [   43    55   682    12    33]\n",
            " [   43    48    17  1353    26]\n",
            " [  112    18    34    15  1567]]\n",
            "Epoch #6\n",
            "Training..\n",
            "Training loss:  0.03065419900748465\n",
            "Training metrics:\n",
            "\t accuracy :  0.9908269229810506\n",
            "\t f1 :  [0.93246076 0.9984596  0.91891892 0.98997631 0.94519043]\n",
            "\t average f1 :  0.957001203433299\n",
            "\t confusion matrix :  [[  9119    177    183     53    311]\n",
            " [   108 166907     33     13     26]\n",
            " [   173     89   4114     25    122]\n",
            " [    42     31     15  10864     12]\n",
            " [   274     38     86     29   7743]]\n",
            "Validating..\n",
            "Validation loss:  0.09210817781942231\n",
            "Validation metrics:\n",
            "\t accuracy :  0.978108192203405\n",
            "\t f1 :  [0.80964686 0.9949882  0.83455882 0.92432432 0.87993139]\n",
            "\t average f1 :  0.8886899194330224\n",
            "\t confusion matrix :  [[ 1410   137    58    51   140]\n",
            " [   61 39011    21    28    19]\n",
            " [   41    59   681    14    30]\n",
            " [   33    48    14  1368    24]\n",
            " [  142    20    33    12  1539]]\n",
            "Epoch #7\n",
            "Training..\n",
            "Training loss:  0.024273587528754165\n",
            "Training metrics:\n",
            "\t accuracy :  0.9927568085999161\n",
            "\t f1 :  [0.94597474 0.99865728 0.93848739 0.99364106 0.95692895]\n",
            "\t average f1 :  0.9667378851095373\n",
            "\t confusion matrix :  [[  9324    167    135     30    238]\n",
            " [   100 166601     29      8     24]\n",
            " [   141     65   4188     17     84]\n",
            " [    34     17      8  10860      9]\n",
            " [   220     38     70     16   7765]]\n",
            "Validating..\n",
            "Validation loss:  0.09679718688130379\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9780415166466639\n",
            "\t f1 :  [0.81023577 0.99470697 0.84197377 0.91655541 0.88654504]\n",
            "\t average f1 :  0.8900033902831416\n",
            "\t confusion matrix :  [[ 1409   141    50    65   131]\n",
            " [   74 38995    19    31    21]\n",
            " [   40    59   674    22    30]\n",
            " [   26    51    12  1373    25]\n",
            " [  133    19    21    18  1555]]\n",
            "Epoch #8\n",
            "Training..\n",
            "Training loss:  0.01998024968499387\n",
            "Training metrics:\n",
            "\t accuracy :  0.9938528069351923\n",
            "\t f1 :  [0.95425036 0.99896863 0.94986631 0.99497992 0.9599218 ]\n",
            "\t average f1 :  0.9715974021173895\n",
            "\t confusion matrix :  [[  9407    116    106     24    221]\n",
            " [    85 166596     21      6     22]\n",
            " [   108     57   4263      8     82]\n",
            " [    27     12      5  10901     10]\n",
            " [   215     25     63     18   7856]]\n",
            "Validating..\n",
            "Validation loss:  0.10416466370224953\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9759301240165356\n",
            "\t f1 :  [0.79021188 0.99412801 0.80131363 0.92582418 0.88089888]\n",
            "\t average f1 :  0.8784753135150634\n",
            "\t confusion matrix :  [[ 1324   137   114    41   180]\n",
            " [   91 38939    69    20    21]\n",
            " [   19    49   732     6    19]\n",
            " [   32    53    28  1348    26]\n",
            " [   89    20    59    10  1568]]\n",
            "Epoch #9\n",
            "Training..\n",
            "Training loss:  0.01719191418615756\n",
            "Training metrics:\n",
            "\t accuracy :  0.9945441228494128\n",
            "\t f1 :  [0.95787185 0.99912091 0.95416298 0.99545726 0.96639785]\n",
            "\t average f1 :  0.9746021671893563\n",
            "\t confusion matrix :  [[  9470    100    114     25    197]\n",
            " [    71 167071     24      5     19]\n",
            " [   112     42   4309      5     58]\n",
            " [    29     13      5  10847      8]\n",
            " [   185     20     54      9   7909]]\n",
            "Validating..\n",
            "Validation loss:  0.09942311208162989\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9785082455438503\n",
            "\t f1 :  [0.81332165 0.99468214 0.84936479 0.91943128 0.88964155]\n",
            "\t average f1 :  0.8932882810390851\n",
            "\t confusion matrix :  [[ 1392   142    58    56   148]\n",
            " [   69 38999    26    27    19]\n",
            " [   31    52   702    12    28]\n",
            " [   30    62    11  1358    26]\n",
            " [  105    20    31    14  1576]]\n",
            "Epoch #10\n",
            "Training..\n",
            "Training loss:  0.013404226792907273\n",
            "Training metrics:\n",
            "\t accuracy :  0.9958799557605891\n",
            "\t f1 :  [0.96806347 0.9992914  0.97034742 0.99667381 0.97258262]\n",
            "\t average f1 :  0.981391742816831\n",
            "\t confusion matrix :  [[  9518     86     75     16    169]\n",
            " [    57 167113     17      5     21]\n",
            " [    61     29   4385      3     48]\n",
            " [    19      7      4  10937     10]\n",
            " [   145     15     31      9   7946]]\n",
            "Validating..\n",
            "Validation loss:  0.10889692498104912\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9750855669644841\n",
            "\t f1 :  [0.79498364 0.99349662 0.81599059 0.91644022 0.87617371]\n",
            "\t average f1 :  0.8794169565868504\n",
            "\t confusion matrix :  [[ 1458   122    63    42   111]\n",
            " [  145 38879    55    39    22]\n",
            " [   45    57   694    11    18]\n",
            " [   53    54    13  1349    18]\n",
            " [  171    15    51    16  1493]]\n",
            "Epoch #11\n",
            "Training..\n",
            "Training loss:  0.019756361980129172\n",
            "Training metrics:\n",
            "\t accuracy :  0.9937610030272351\n",
            "\t f1 :  [0.96383824 0.99816845 0.95781859 0.98820949 0.96685184]\n",
            "\t average f1 :  0.9749773216874198\n",
            "\t confusion matrix :  [[  9462    127     71     16    161]\n",
            " [    87 166766     38     78     43]\n",
            " [    63     83   4303      9     63]\n",
            " [    24    105      7  10812      8]\n",
            " [   161     51     45     11   7919]]\n",
            "Validating..\n",
            "Validation loss:  0.10233647961701665\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9781748677601458\n",
            "\t f1 :  [0.80640549 0.99471966 0.84606061 0.92438115 0.88646789]\n",
            "\t average f1 :  0.891606957940162\n",
            "\t confusion matrix :  [[ 1410   137    62    52   135]\n",
            " [   81 38995    29    17    18]\n",
            " [   34    57   698    12    24]\n",
            " [   37    59     9  1363    19]\n",
            " [  139    16    27    18  1546]]\n",
            "Epoch #12\n",
            "Training..\n",
            "Training loss:  0.0105288064762674\n",
            "Training metrics:\n",
            "\t accuracy :  0.9966932338477191\n",
            "\t f1 :  [0.97402267 0.99942815 0.9786811  0.99716584 0.97748765]\n",
            "\t average f1 :  0.9853570829875314\n",
            "\t confusion matrix :  [[  9580     70     50     11    149]\n",
            " [    42 166907     18      6     15]\n",
            " [    42     20   4430      4     30]\n",
            " [    18      9      4  10907      6]\n",
            " [   129     11     25      4   8011]]\n",
            "Validating..\n",
            "Validation loss:  0.10557415868554797\n",
            "Validation metrics:\n",
            "\t accuracy :  0.978263768502467\n",
            "\t f1 :  [0.80831409 0.99465732 0.84441733 0.92465053 0.88743975]\n",
            "\t average f1 :  0.8918958020541707\n",
            "\t confusion matrix :  [[ 1400   138    58    52   148]\n",
            " [   75 39003    27    14    21]\n",
            " [   34    61   692    11    27]\n",
            " [   36    63    12  1356    20]\n",
            " [  123    20    25    13  1565]]\n",
            "Epoch #13\n",
            "Training..\n",
            "Training loss:  0.008944040423052179\n",
            "Training metrics:\n",
            "\t accuracy :  0.99721018904837\n",
            "\t f1 :  [0.97751155 0.99953861 0.98449612 0.99785182 0.97958685]\n",
            "\t average f1 :  0.9877969903506214\n",
            "\t confusion matrix :  [[  9628     57     37     10    138]\n",
            " [    37 166810     12      2     16]\n",
            " [    31     14   4445      3     27]\n",
            " [    14      5      3  10916      4]\n",
            " [   119     11     13      6   8014]]\n",
            "Validating..\n",
            "Validation loss:  0.1117703446320125\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9778637151620216\n",
            "\t f1 :  [0.80838839 0.99430319 0.84268293 0.92407661 0.88556642]\n",
            "\t average f1 :  0.8910035076687096\n",
            "\t confusion matrix :  [[ 1407   159    56    46   128]\n",
            " [   70 39009    23    17    21]\n",
            " [   40    62   691     9    23]\n",
            " [   36    73     7  1351    20]\n",
            " [  132    22    38    14  1540]]\n",
            "Epoch #14\n",
            "Training..\n",
            "Training loss:  0.008934240874454932\n",
            "Training metrics:\n",
            "\t accuracy :  0.9971196204676381\n",
            "\t f1 :  [0.97735906 0.99956624 0.98324884 0.99776307 0.97776689]\n",
            "\t average f1 :  0.9871408209597915\n",
            "\t confusion matrix :  [[  9648     57     27     13    135]\n",
            " [    34 167071      6      2     15]\n",
            " [    34     15   4461      4     26]\n",
            " [    14      5      4  10928      3]\n",
            " [   133     11     36      4   7982]]\n",
            "Validating..\n",
            "Validation loss:  0.11869487592152186\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9765968795839445\n",
            "\t f1 :  [0.78849722 0.99421317 0.84041898 0.91882789 0.87378641]\n",
            "\t average f1 :  0.8831487319433569\n",
            "\t confusion matrix :  [[ 1275   170    59    64   228]\n",
            " [   41 39000    28    27    44]\n",
            " [   29    61   682    12    41]\n",
            " [   20    65     9  1364    29]\n",
            " [   73    18    20    15  1620]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "## TODO: Re-run with unidirectional LSTMs\n",
        "## Keep other hyperparameters fixed\n",
        "train_val_loop_lstm({\n",
        "    \"bidirectional\": False,\n",
        "    \"batch_size\": 512,\n",
        "    \"d_emb\": 64,\n",
        "    \"d_hidden\": 128,\n",
        "    \"num_epochs\": 15,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"l2\": 1e-6,\n",
        "})\n",
        "## END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8UChDyKaPBs"
      },
      "source": [
        "### Questions **(2 points)**\n",
        "\n",
        "(a) How does the final performance of LSTMs compare to FFNNs? Is it better? What is a possible explanation?\n",
        "\n",
        "**TODO: Please fill in your answer here**\n",
        "\n",
        "(b) How does bidirectional LSTMs compare to unidirectional LSTMs? Why?\n",
        "\n",
        "**TODO: Please fill in your answer here**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kMEWxkN_bpIT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "92aa20ad-3b55-4b75-de29-c8efb0e577c0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unmatched ')' (<ipython-input-19-fe9ac37f6b3d>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-fe9ac37f6b3d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    a) LSTMs perform better than FFNs, as they can capture sequntial dependancies in the data , and can\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
          ]
        }
      ],
      "source": [
        "a) LSTMs perform better than FFNs, as they can capture sequntial dependancies in the data , and can\n",
        "keep track of the context while the FFNs treat each token individually\n",
        "b) Bidirectional LSTMs perform better than unidirectional LSTMs according to the F1 scores shown above,\n",
        "as they can capture inforrmation from both past and future tokens, so they use the entire input sequence."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}